{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35842308-3e4d-4f13-a1f1-1a522150932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Found file corresponding to this W&B run with: `grep \"3pzwny4n\" outputs/*/args.json`\n",
    "model_folder = '../outputs/bytedance-20211130-181619/'\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721d166f-ff86-40ba-ba96-89088208c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/transcription/contrastive-pitch-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd48d30-8002-452a-bc24-ba950729dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bytedance import Bytedance_Regress_pedal_Notes\n",
    "from models.contrastive import ContrastiveModel\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "def get_model():\n",
    "    num_output_nodes = 256 # contrastive embedding dim\n",
    "    out_activation = None\n",
    "    \n",
    "    model = Bytedance_Regress_pedal_Notes(\n",
    "        num_output_nodes, out_activation, tiny=False\n",
    "    )\n",
    "    \n",
    "    return ContrastiveModel(model, min_midi, max_midi, num_output_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f57072d-4ca0-4b8d-aba6-42abdb282637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from: ../outputs/bytedance-20211130-181619/59_epochs.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import natsort\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))\n",
    "\n",
    "model_path = natsort.natsorted(model_paths)[-2]\n",
    "print('loaded model from:', model_path)\n",
    "\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load(model_path)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40498da1-8a93-4878-ae67-1479cd71db3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 993 tracks\n",
      "loaded audio batch of shape: torch.Size([256, 16000]) with labels torch.Size([256, 88])\n"
     ]
    }
   ],
   "source": [
    "# from dataloader.nsynth import load_nsynth\n",
    "# dataset = load_nsynth('test', 'keyboard')\n",
    "\n",
    "from dataloader.nsynth_chords import load_nsynth_chords\n",
    "dataset = load_nsynth_chords('test')\n",
    "\n",
    "print('loaded', len(dataset), 'tracks')\n",
    "\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "import numpy as np\n",
    "from utils.misc import midi_vals_to_categorical, hz_to_midi_v\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "all_midis = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    track = dataset[i]\n",
    "    start_idx = 0\n",
    "    end_idx = 16_000\n",
    "    #\n",
    "    audio = torch.tensor(track.waveform[start_idx : end_idx], dtype=torch.float32)\n",
    "    x.append(audio)\n",
    "    #\n",
    "    frequencies = track.get_frequencies_from_offset(start_idx, end_idx)\n",
    "    midis = np.rint(hz_to_midi_v(frequencies))\n",
    "    all_midis.append(list(midis))\n",
    "    categorical = midi_vals_to_categorical(midis, min_midi, max_midi)\n",
    "    y.append(torch.tensor(categorical, dtype=torch.float32))\n",
    "x = torch.stack(x)\n",
    "y = torch.stack(y)\n",
    "print('loaded audio batch of shape:', x.shape, 'with labels', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4685a6-932d-45ed-9895-41997e2692e2",
   "metadata": {},
   "source": [
    "# Inference (val set) - calculating p(chord|audio)\n",
    "\n",
    "Beam search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712381a0-635a-49b1-82fd-58d77681b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = torch.nn.CosineSimilarity(1)\n",
    "def find_best_chord(audio, eps=0.05):\n",
    "    audio_encoding = model(audio[None])\n",
    "    best_labels = torch.zeros((88))\n",
    "    zero_label_encoding = model.encode_note_labels(best_labels[None])\n",
    "    best_overall_sim = cos_sim(audio_encoding.squeeze(), zero_label_encoding).item()\n",
    "    for _ in range(6):\n",
    "        new_labels = best_labels.repeat((88,1))\n",
    "        new_notes = torch.eye(88)\n",
    "        new_labels = torch.maximum(new_notes, new_labels) # 88 tensors, each one has a new 1 at a different position\n",
    "        label_encodings = model.encode_note_labels(new_labels)\n",
    "        cos_sims = cos_sim(audio_encoding, label_encodings)\n",
    "        best_idx = cos_sims.argmax()\n",
    "        best_sim = cos_sims[best_idx].item()\n",
    "        \n",
    "        if best_sim - best_overall_sim > eps:\n",
    "            #print('choosing note', note)\n",
    "            best_overall_sim = best_sim\n",
    "            best_labels = new_labels[best_idx]\n",
    "        else:\n",
    "            #print(f'breaking after {_} steps')\n",
    "            break\n",
    "        # print('**'*40)\n",
    "    return best_sim, best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7335757c-38c2-4149-8e04-28dd31576394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best chord: 100%|██████████| 256/256 [00:39<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "y_pred_sim = []\n",
    "y_true_sim = []\n",
    "y_pred = []\n",
    "#\n",
    "for audio, y_true in tqdm.tqdm(zip(x, y), desc='Finding best chord', total=len(x)):\n",
    "    sim, label = find_best_chord(audio)\n",
    "    y_pred_sim.append(sim)\n",
    "    y_pred.append(label)\n",
    "    #\n",
    "    y_true_enc = model.encode_note_labels(y_true[None])\n",
    "    audio_enc = model(audio[None]).squeeze()\n",
    "    y_true_sim.append(cos_sim(y_true_enc, audio_enc).item())\n",
    "y_pred_sim = torch.tensor(np.array(y_pred_sim))\n",
    "y_pred = torch.stack(y_pred)\n",
    "y_true_sim = torch.tensor(np.array(y_true_sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83fd052a-7ff0-450d-bcb5-e7b700c05df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0195)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics import (\n",
    "    categorical_accuracy, pitch_number_acc, NStringChordAccuracy,\n",
    "    precision, recall, f1\n",
    ")\n",
    "\n",
    "categorical_accuracy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ecd5f3f-4416-4f1a-b69c-09eac8a05943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1325)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3711b0-cf73-473b-aa41-8cabc1e6bef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1116)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "264a7cbd-e32e-41de-8098-d418630d6120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1633)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53373dd4-ab53-4a92-8171-b82842f1ce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_pred_sim</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_true_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[65]</td>\n",
       "      <td>0.915799</td>\n",
       "      <td>[65]</td>\n",
       "      <td>0.860138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25, 50]</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>[62]</td>\n",
       "      <td>0.723862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[23, 46]</td>\n",
       "      <td>0.940088</td>\n",
       "      <td>[37]</td>\n",
       "      <td>0.724195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[63, 77]</td>\n",
       "      <td>0.938952</td>\n",
       "      <td>[63]</td>\n",
       "      <td>0.803312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[23, 58]</td>\n",
       "      <td>0.930739</td>\n",
       "      <td>[57]</td>\n",
       "      <td>0.835980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred  y_pred_sim y_true  y_true_sim\n",
       "0      [65]    0.915799   [65]    0.860138\n",
       "1  [25, 50]    0.923529   [62]    0.723862\n",
       "2  [23, 46]    0.940088   [37]    0.724195\n",
       "3  [63, 77]    0.938952   [63]    0.803312\n",
       "4  [23, 58]    0.930739   [57]    0.835980"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_note_vals = [str((label.nonzero() + min_midi).flatten().tolist()) for label in y]\n",
    "y_pred_note_vals = [str((label.nonzero() + min_midi).flatten().tolist()) for label in y_pred]\n",
    "\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame({\n",
    "    'y_pred': y_pred_note_vals,\n",
    "    'y_pred_sim': y_pred_sim,\n",
    "    'y_true': y_true_note_vals,\n",
    "    'y_true_sim': y_true_sim,\n",
    "})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674c431-224e-4658-b665-ea94ecb4daa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82218ed-3cff-45ec-b3db-62d43eada6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
