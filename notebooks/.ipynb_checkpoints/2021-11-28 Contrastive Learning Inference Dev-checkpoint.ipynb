{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f81ef8c-8684-4664-b491-a5fb5f5d770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2a519a-c0c0-4bd5-896f-27a9667c969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/jxm3/research/transcription/contrastive-pitch-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a29a10-556c-4ff9-bff3-c35316fb9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.crepe import CREPE\n",
    "from models.contrastive import ContrastiveModel\n",
    "\n",
    "def get_model():\n",
    "    # TODO(jxm): support nn.DataParallel here\n",
    "    num_output_nodes = 256 # contrastive embedding dim\n",
    "    out_activation = 'sigmoid'\n",
    "    \n",
    "    model = CREPE(\n",
    "        model='tiny',\n",
    "        num_output_nodes=num_output_nodes, \n",
    "        load_pretrained=False,\n",
    "        out_activation=out_activation\n",
    "    )\n",
    "    \n",
    "    min_midi = 21\n",
    "    max_midi = 108\n",
    "    return ContrastiveModel(model, min_midi, max_midi, num_output_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddec302-6dfa-423b-b9a3-e4a326378330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3009cfa2-c657-414a-9356-b17987893eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastiveModel(\n",
      "  (embedding): Embedding(88, 256)\n",
      "  (model): CREPE(\n",
      "    (conv1): Conv2d(1, 128, kernel_size=(512, 1), stride=(4, 1))\n",
      "    (conv1_BN): BatchNorm2d(128, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 16, kernel_size=(64, 1), stride=(1, 1))\n",
      "    (conv2_BN): BatchNorm2d(16, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(16, 16, kernel_size=(64, 1), stride=(1, 1))\n",
      "    (conv3_BN): BatchNorm2d(16, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(16, 16, kernel_size=(64, 1), stride=(1, 1))\n",
      "    (conv4_BN): BatchNorm2d(16, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(16, 32, kernel_size=(64, 1), stride=(1, 1))\n",
      "    (conv5_BN): BatchNorm2d(32, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (conv6): Conv2d(32, 64, kernel_size=(64, 1), stride=(1, 1))\n",
      "    (conv6_BN): BatchNorm2d(64, eps=0.0010000000474974513, momentum=0.0, affine=True, track_running_stats=True)\n",
      "    (classifier): Linear(in_features=256, out_features=360, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (classifier_out): Linear(in_features=360, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fd5ad4-6c2f-410a-8508-7ae583cc7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.nsynth import load_nsynth\n",
    "\n",
    "dataset = load_nsynth('test', 'keyboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e81983a8-9bf2-423c-b030-a694f857d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "audio = torch.tensor(dataset[0].waveform[None, :16000], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "432af929-6747-4f3c-872a-ea882dc92951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0838, 0.4255, 0.4623, 0.0677, 0.7165])\n",
      "Similarity at step 0: 0.029\n",
      "\t False True\n",
      "Similarity at step 1: 0.463\n",
      "\t False True\n",
      "Similarity at step 2: 0.512\n",
      "\t False True\n",
      "Similarity at step 3: 0.540\n",
      "\t False True\n",
      "Similarity at step 4: 0.556\n",
      "\t False True\n",
      "Similarity at step 5: 0.565\n",
      "\t False True\n",
      "Similarity at step 6: 0.570\n",
      "\t False True\n",
      "Similarity at step 7: 0.574\n",
      "\t False True\n",
      "Similarity at step 8: 0.576\n",
      "\t False True\n",
      "Similarity at step 9: 0.578\n",
      "\t False True\n",
      "tensor([0.0838, 0.4255, 0.4623, 0.0677, 0.7165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-4bb59f831638>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor((labels - grad_step_size * labels.grad), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "##with torch.no_grad():\n",
    "##    audio_rep = model(x)\n",
    "audio_rep = torch.rand((1, 256), dtype=torch.float32, requires_grad=False)\n",
    "n_steps = 10\n",
    "grad_step_size = 100\n",
    "\n",
    "labels = torch.rand((1, 88), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "print(audio_rep[0, :5])\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "for _ in range(n_steps):\n",
    "    label_rep = labels @ model.embedding.weight\n",
    "    print(f'Similarity at step {_}: {cos_sim(audio_rep, label_rep).item():.3f}')\n",
    "    loss = 1 - cos_sim(audio_rep, label_rep)\n",
    "    loss.backward()\n",
    "    print('\\t',labels.grad is None, label_rep.grad is None)\n",
    "    labels = torch.tensor((labels - grad_step_size * labels.grad), requires_grad=True)\n",
    "print(audio_rep[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec0e325e-5656-4759-9c72-ef4251904c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3260, 0.0078, 0.5946, 0.6747, 0.9492])\n",
      "Similarity at step 0: 0.002\n",
      "\t False True\n",
      "Similarity at step 1: 0.094\n",
      "\t False True\n",
      "Similarity at step 2: 0.310\n",
      "\t False True\n",
      "Similarity at step 3: 0.483\n",
      "\t False True\n",
      "Similarity at step 4: 0.543\n",
      "\t False True\n",
      "Similarity at step 5: 0.560\n",
      "\t False True\n",
      "Similarity at step 6: 0.565\n",
      "\t False True\n",
      "Similarity at step 7: 0.566\n",
      "\t False True\n",
      "Similarity at step 8: 0.567\n",
      "\t False True\n",
      "Similarity at step 9: 0.567\n",
      "\t False True\n",
      "tensor([0.3260, 0.0078, 0.5946, 0.6747, 0.9492])\n"
     ]
    }
   ],
   "source": [
    "audio_rep = torch.rand((1, 256), dtype=torch.float32, requires_grad=False)\n",
    "n_steps = 10\n",
    "\n",
    "labels = torch.rand((1, 88), dtype=torch.float32, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([labels], lr=5.0, momentum=0.9)\n",
    "\n",
    "print(audio_rep[0, :5])\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "for _ in range(n_steps):\n",
    "    label_rep = labels @ model.embedding.weight\n",
    "    print(f'Similarity at step {_}: {cos_sim(audio_rep, label_rep).item():.3f}')\n",
    "    loss = 1 - cos_sim(audio_rep, label_rep)\n",
    "    loss.backward()\n",
    "    print('\\t',labels.grad is None, label_rep.grad is None)\n",
    "    optimizer.step()\n",
    "print(audio_rep[0, :5]) # just to prove that nothing changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc765d-5f07-4786-b89a-d05d12de34b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
