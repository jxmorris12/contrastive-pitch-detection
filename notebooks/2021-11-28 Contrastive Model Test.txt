{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35842308-3e4d-4f13-a1f1-1a522150932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# I first used this notebook with this model, which accidentally\n",
    "# had a sigmoid output on the audio representations.\n",
    "# model_folder = '../outputs/crepe-20211128-170108'\n",
    "\n",
    "# Now i'm trying this one.\n",
    "model_folder = '../outputs/crepe-20211128-195735'\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721d166f-ff86-40ba-ba96-89088208c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/transcription/contrastive-pitch-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd48d30-8002-452a-bc24-ba950729dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.crepe import CREPE\n",
    "from models.contrastive import ContrastiveModel\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "def get_model():\n",
    "    # TODO(jxm): support nn.DataParallel here\n",
    "    num_output_nodes = 256 # contrastive embedding dim\n",
    "    out_activation = None\n",
    "    \n",
    "    model = CREPE(\n",
    "        model='tiny',\n",
    "        num_output_nodes=num_output_nodes, \n",
    "        load_pretrained=False,\n",
    "        out_activation=out_activation\n",
    "    )\n",
    "    \n",
    "    return ContrastiveModel(model, min_midi, max_midi, num_output_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783dafb-10bd-46c8-8372-5f8fd11b930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57072d-4ca0-4b8d-aba6-42abdb282637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import natsort\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))\n",
    "\n",
    "model_path = natsort.natsorted(model_paths)[-2]\n",
    "print('loaded model from:', model_path)\n",
    "#model.load_state_dict(torch.load(model_path)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b703d-e601-4a27-83ba-e9f3671ed9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader.nsynth import load_nsynth\n",
    "# dataset = load_nsynth('test', 'keyboard')\n",
    "\n",
    "from dataloader.nsynth_chords import load_nsynth_chords\n",
    "dataset = load_nsynth_chords('test')\n",
    "\n",
    "print('loaded', len(dataset), 'tracks')\n",
    "\n",
    "import random\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c18b1-1355-4ff8-89ba-0aa2b06ff48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils.misc import midi_vals_to_categorical, hz_to_midi_v\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "all_midis = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    track = dataset[i]\n",
    "    start_idx = 0\n",
    "    end_idx = 16_000\n",
    "    #\n",
    "    audio = torch.tensor(track.waveform[start_idx : end_idx], dtype=torch.float32)\n",
    "    x.append(audio)\n",
    "    #\n",
    "    frequencies = track.get_frequencies_from_offset(start_idx, end_idx)\n",
    "    midis = np.rint(hz_to_midi_v(frequencies))\n",
    "    all_midis.append(list(midis))\n",
    "    categorical = midi_vals_to_categorical(midis, min_midi, max_midi)\n",
    "    y.append(torch.tensor(categorical, dtype=torch.float32))\n",
    "x = torch.stack(x)\n",
    "y = torch.stack(y)\n",
    "print('loaded audio batch of shape:', x.shape, 'with labels', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabf6d9-c58a-4722-8fd8-f6de26879324",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96900b-097a-4743-a2b5-a65618d40337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=x[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d363c-eb59-43b3-9d39-0bc3950a4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_midis_str = [str(m) for m in all_midis]\n",
    "print(len(all_midis_str))\n",
    "print(len(set(all_midis_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40498da1-8a93-4878-ae67-1479cd71db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = model(x)\n",
    "note_labels = y\n",
    "\n",
    "#### Taken from ContrastiveModel.contrastive_loss() ####\n",
    "batch_size, num_notes = note_labels.shape\n",
    "assert num_notes == model.num_labels\n",
    "chord_embeddings = note_labels @ model.embedding.weight # [b,n] @ [n, d] -> [b, d]\n",
    "# Make sure the shapes match up.\n",
    "assert chord_embeddings.shape == audio_embeddings.shape\n",
    "# Normalize to create embeddings.\n",
    "# TODO(jxm) should I do bilinear interpolation here?\n",
    "normalized_audio_embeddings = audio_embeddings / torch.norm(audio_embeddings, dim=1, keepdim=True)\n",
    "normalized_chord_embeddings = chord_embeddings / torch.norm(chord_embeddings, dim=1, keepdim=True)\n",
    "logits = (normalized_audio_embeddings @ normalized_chord_embeddings.T) * np.exp(model.temperature)\n",
    "# Symmetric loss function\n",
    "labels = torch.diag(torch.ones(batch_size)).to(logits.device) # Identity matrix\n",
    "loss_a = torch.nn.functional.binary_cross_entropy_with_logits(labels, logits)\n",
    "loss_n = torch.nn.functional.binary_cross_entropy_with_logits(labels, logits.T)\n",
    "####\n",
    "\n",
    "loss = (loss_a + loss_n)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8db89e-5140-4be8-86f5-10e49dcfaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_a, loss_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89b372-544c-4cf8-9bd3-f361ea53435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(labels, logits+labels*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838cdd4-9dbd-4698-b2f8-46b5e93f303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(logits.detach()) # true model output (no softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71371340-2418-458e-92b9-8dc6923dc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7591da-df14-4d4f-9ef3-44141e0c5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(labels.detach()) # desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcf935-1f90-40b2-8e41-4f01ddb8347f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891d3f4-fd59-47c6-84c0-0438df7855cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(torch.nn.functional.softmax(logits.detach(), 1)) # true model output (with softmax over dim 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cfe69-2532-494d-90e5-1f7cb0f4e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(torch.nn.functional.softmax(logits.detach(), 0)) # true model output (with softmax over dim 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478c6ad-c249-42de-9b8b-2b53f573e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e03a88-13b4-4d14-88a7-a40540555e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_logits = (audio_embeddings @ chord_embeddings.T) * np.exp(model.temperature)\n",
    "import seaborn as sns\n",
    "sns.heatmap(unnorm_logits.detach()) # true model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc0ce7-fd53-45e8-81df-1d1e38aa30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_notes, emb_dim = chord_embeddings.shape\n",
    "# create the TSNE\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0, perplexity=5, learning_rate='auto', init='random')\n",
    "emb_dim_2 = tsne.fit_transform(chord_embeddings.detach())\n",
    "# 12 colors, one per note\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', \n",
    "          '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#0e3f43', '#e0e0e0']\n",
    "emb_colors = []\n",
    "for i in range(num_notes):\n",
    "    emb_colors.append(colors[round(y[i].sum().item())])\n",
    "# scatterplot of TSNE results\n",
    "plt.scatter(emb_dim_2[:,0], emb_dim_2[:,1], c=emb_colors) # Chord embeddings colored by number of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952058dc-477b-4d63-ac64-75da60092da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_notes, emb_dim = chord_embeddings.shape\n",
    "# create the TSNE\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0, perplexity=5, learning_rate='auto', init='random')\n",
    "emb_dim_2 = tsne.fit_transform(audio_embeddings.detach())\n",
    "# 12 colors, one per note\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', \n",
    "          '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#0e3f43', '#e0e0e0']\n",
    "emb_colors = []\n",
    "for i in range(num_notes):\n",
    "    emb_colors.append(colors[round(y[i].sum().item())])\n",
    "# scatterplot of TSNE results\n",
    "plt.scatter(emb_dim_2[:,0], emb_dim_2[:,1], c=emb_colors) # Audio embeddings colored by number of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5fea1-42a8-4a44-8c02-a59918213921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_embeddings = torch.cat([audio_embeddings, chord_embeddings]).detach()\n",
    "num_notes, emb_dim = chord_embeddings.shape\n",
    "# create the TSNE\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0, perplexity=5, learning_rate='auto', init='random')\n",
    "emb_dim_2 = tsne.fit_transform(all_embeddings)\n",
    "# 12 colors, one per note\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', \n",
    "          '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#0e3f43', '#e0e0e0']\n",
    "emb_colors = []\n",
    "for i in range(len(all_embeddings)):\n",
    "    ci = 0 if i < num_notes else 1 # make chord and audio embeddings different colors\n",
    "    emb_colors.append(colors[ci])\n",
    "# scatterplot of TSNE results\n",
    "plt.scatter(emb_dim_2[:,0], emb_dim_2[:,1], c=emb_colors) # Audio vs. chord embeddings. (audio are blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd624df-0c74-4c39-b0f8-d510585ffd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([audio_embeddings, chord_embeddings]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911e9b4-c590-4852-a200-e963cff74e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.norm(audio_embeddings, p=2, dim=1).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784b686-cb01-4305-9d18-202b33188192",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2dea0-4024-4e31-87bb-d8abdc3aa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe146703-83ef-44f4-ae98-56a02d61944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.norm(chord_embeddings, p=2, dim=1).detach()) # these r way smaller, \n",
    "                                    # and clearly have magnitude proportional to the number of notes in the chord..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a174f-8563-41e0-941f-b52d78b1d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5cd4f-a979-4e26-b316-fc8827b558ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5fa4b-29d9-486d-98eb-2f2782ee9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_embeddings.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740e5fd-d0cd-4207-b5d1-6b955d0d9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_embeddings.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707e9e5-c423-49c6-88f3-62a48d8fff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee70648-ade2-47aa-ad07-8d9226494c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.xavier_uniform_(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e49f2-20bc-43bc-a3d0-914f57b88012",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0125fdf-711c-4d80-8d3f-e781d11f9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to see what the output of a random linear layer is. Is it [0,1]?\n",
    "x = torch.nn.Linear(in_features=360, out_features=10)\n",
    "x(torch.randn((16,360)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2e65e-557b-493d-a4a0-6809e69acd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(chord_embeddings.flatten().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c02940-398e-40a1-98dd-84a79e99a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(audio_embeddings.flatten().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c39433-b717-48fc-9ba5-666720374712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "softmax = functools.partial(torch.nn.functional.softmax, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea33558-312e-46fe-9622-57a201e0d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296235b4-39c0-4a2d-93f9-4f6c09d51e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66464f7d-8fb0-4481-82f9-9f64d0db5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "torch.unsqueeze(cos_sim(audio_embeddings[14], chord_embeddings), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cf6b6-a307-4e65-8471-4325c2fd7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the loss for uniform logits?\n",
    "\n",
    "logits = torch.ones((batch_size, batch_size), dtype=torch.float32) * 3000\n",
    "# Symmetric loss function\n",
    "labels = torch.diag(torch.ones(batch_size)).to(logits.device) # Identity matrix\n",
    "loss_a = torch.nn.functional.binary_cross_entropy_with_logits(labels, logits)\n",
    "loss_n = torch.nn.functional.binary_cross_entropy_with_logits(labels, logits.T)\n",
    "####\n",
    "\n",
    "loss = (loss_a + loss_n)/2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754a5ae-37f1-4d46-928a-ae9176b8410a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
