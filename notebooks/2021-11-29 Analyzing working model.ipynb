{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35842308-3e4d-4f13-a1f1-1a522150932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Found file corresponding to this W&B run with: `grep \"3pzwny4n\" outputs/*/args.json`\n",
    "model_folder = '../outputs/crepe-20211129-122548'\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721d166f-ff86-40ba-ba96-89088208c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/transcription/contrastive-pitch-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd48d30-8002-452a-bc24-ba950729dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bytedance import Bytedance_Regress_pedal_Notes\n",
    "from models.contrastive import ContrastiveModel\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "def get_model():\n",
    "    num_output_nodes = 256 # contrastive embedding dim\n",
    "    out_activation = None\n",
    "    \n",
    "    model = Bytedance_Regress_pedal_Notes(\n",
    "        num_output_nodes, out_activation, tiny=False\n",
    "    )\n",
    "    \n",
    "    return ContrastiveModel(model, min_midi, max_midi, num_output_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f57072d-4ca0-4b8d-aba6-42abdb282637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from: ../outputs/crepe-20211129-122548/84_epochs.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import natsort\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model_paths = glob.glob(os.path.join(model_folder, '*'))\n",
    "\n",
    "model_path = natsort.natsorted(model_paths)[-2]\n",
    "print('loaded model from:', model_path)\n",
    "\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load(model_path)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496b703d-e601-4a27-83ba-e9f3671ed9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 993 tracks\n"
     ]
    }
   ],
   "source": [
    "# from dataloader.nsynth import load_nsynth\n",
    "# dataset = load_nsynth('test', 'keyboard')\n",
    "\n",
    "from dataloader.nsynth_chords import load_nsynth_chords\n",
    "dataset = load_nsynth_chords('test')\n",
    "\n",
    "print('loaded', len(dataset), 'tracks')\n",
    "\n",
    "import random\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6c18b1-1355-4ff8-89ba-0aa2b06ff48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded audio batch of shape: torch.Size([256, 16000]) with labels torch.Size([256, 88])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.misc import midi_vals_to_categorical, hz_to_midi_v\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "min_midi = 21\n",
    "max_midi = 108\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "all_midis = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    track = dataset[i]\n",
    "    start_idx = 0\n",
    "    end_idx = 16_000\n",
    "    #\n",
    "    audio = torch.tensor(track.waveform[start_idx : end_idx], dtype=torch.float32)\n",
    "    x.append(audio)\n",
    "    #\n",
    "    frequencies = track.get_frequencies_from_offset(start_idx, end_idx)\n",
    "    midis = np.rint(hz_to_midi_v(frequencies))\n",
    "    all_midis.append(list(midis))\n",
    "    categorical = midi_vals_to_categorical(midis, min_midi, max_midi)\n",
    "    y.append(torch.tensor(categorical, dtype=torch.float32))\n",
    "x = torch.stack(x)\n",
    "y = torch.stack(y)\n",
    "print('loaded audio batch of shape:', x.shape, 'with labels', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fabf6d9-c58a-4722-8fd8-f6de26879324",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5f29ab836dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# validation set chord distribution (by num. notes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(y.sum(1)) # validation set chord distribution (by num. notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96900b-097a-4743-a2b5-a65618d40337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=x[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40498da1-8a93-4878-ae67-1479cd71db3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1116, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings = model(x)\n",
    "note_labels = y\n",
    "\n",
    "batch_size, num_notes = note_labels.shape\n",
    "assert num_notes == model.num_labels\n",
    "chord_embeddings = model.encode_note_labels(note_labels)\n",
    "assert chord_embeddings.shape == audio_embeddings.shape\n",
    "# Normalize embeddings and compute logits.\n",
    "normalized_audio_embeddings = audio_embeddings / torch.norm(audio_embeddings, p=2, dim=1, keepdim=True)\n",
    "normalized_chord_embeddings = chord_embeddings / torch.norm(chord_embeddings, p=2, dim=1, keepdim=True)\n",
    "unscaled_audio_to_chord_sim = torch.matmul(normalized_audio_embeddings, normalized_chord_embeddings.T)\n",
    "audio_to_chord_sim = unscaled_audio_to_chord_sim * torch.exp(model.temperature)\n",
    "chord_to_audio_sim = audio_to_chord_sim.T\n",
    "\n",
    "logits = unscaled_audio_to_chord_sim\n",
    "\n",
    "# Compute labels when there may be duplicates.\n",
    "labels = (note_labels[:,None] == note_labels).all(2).type(torch.float32)\n",
    "labels = labels / labels.sum(1)\n",
    "# Compute loss across both axes.\n",
    "loss_a = torch.nn.functional.cross_entropy(audio_to_chord_sim, labels)\n",
    "loss_n = torch.nn.functional.cross_entropy(chord_to_audio_sim, labels.T)\n",
    "loss = (loss_a + loss_n)/2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffe592-ca8e-49dc-8005-632010a73a69",
   "metadata": {},
   "source": [
    "# Investigating train/val difference\n",
    "\n",
    "Why is there such a big difference between the train and validation loss? Clearly, the heatmap above is pretty bad. But what does it look like for the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eedf898-05b7-4350-8fdf-9f2a02d40e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 0 tracks with 1000 fake NSynth chords\n",
      "--> MusicDataLoader loading dataset nsynth_keyboard_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling tracks: 100%|██████████| 51821/51821 [00:00<00:00, 1481928.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackFrameSampler loaded 4000 frames\n"
     ]
    }
   ],
   "source": [
    "from generator import AudioDataGenerator\n",
    "g = AudioDataGenerator(\n",
    "        [], 16000, float('inf'),\n",
    "        randomize_train_frame_offsets=True,\n",
    "        batch_size=256,\n",
    "        augmenter=None,\n",
    "        normalize_audio=False,\n",
    "        label_format='categorical',\n",
    "        min_midi=21, max_midi=108,\n",
    "        sample_rate=16000,\n",
    "        batch_by_track=False,\n",
    "        num_fake_nsynth_chords=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2953e090-a55a-41e8-8cb5-89362ceda0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7aaf1-e3d0-4cfa-a1ba-b918d2801edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2072, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings = model(x_train)\n",
    "note_labels = y_train\n",
    "\n",
    "batch_size, num_notes = note_labels.shape\n",
    "assert num_notes == model.num_labels\n",
    "chord_embeddings = model.encode_note_labels(note_labels)\n",
    "assert chord_embeddings.shape == audio_embeddings.shape\n",
    "# Normalize embeddings and compute logits.\n",
    "normalized_audio_embeddings = audio_embeddings / torch.norm(audio_embeddings, p=2, dim=1, keepdim=True)\n",
    "normalized_chord_embeddings = chord_embeddings / torch.norm(chord_embeddings, p=2, dim=1, keepdim=True)\n",
    "unscaled_audio_to_chord_sim = torch.matmul(normalized_audio_embeddings, normalized_chord_embeddings.T)\n",
    "audio_to_chord_sim = unscaled_audio_to_chord_sim * torch.exp(model.temperature)\n",
    "chord_to_audio_sim = audio_to_chord_sim.T\n",
    "\n",
    "logits = unscaled_audio_to_chord_sim\n",
    "\n",
    "# Compute labels when there may be duplicates.\n",
    "labels = (note_labels[:,None] == note_labels).all(2).type(torch.float32)\n",
    "labels = labels / labels.sum(1)\n",
    "# Compute loss across both axes.\n",
    "loss_a = torch.nn.functional.cross_entropy(audio_to_chord_sim, labels)\n",
    "loss_n = torch.nn.functional.cross_entropy(chord_to_audio_sim, labels.T)\n",
    "loss = (loss_a + loss_n)/2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fddf46e0-5554-44a2-8a67-367d2f0e2b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 4948, 2: 2563, 3: 1256, 4: 599, 5: 340, 6: 294}\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for _ in range(10000):\n",
    "    a.append(np.random.choice([1,2,3,4,5,6], p=[0.5, 0.25, 0.125, 0.0625, 0.03125, 0.03125]))\n",
    "    \n",
    "import collections\n",
    "import pprint\n",
    "pprint.pprint( dict( collections.Counter(a) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712381a0-635a-49b1-82fd-58d77681b30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
